# Deep Learning Notes
This respository contains short notes on Deep Learning research publications that I read.   
This idea was borrowed from [Denny Britz](https://github.com/dennybritz) and his [notes repository](https://github.com/dennybritz/deeplearning-papernotes).

## Natural Language Processing
#### Word Embeddings
[Linguistic Regularities in Continuous Space Word Representations](notes/regularities-in-cont-word-repr.md) - Mikolov et al. [[paper](https://www.aclweb.org/anthology/N13-1090)]   
[Efficient Estimation of Word Representations in Vector Space](notes/efficient-esti-of-word-repr.md) - Mikolov et al. [[paper](https://arxiv.org/abs/1301.3781)]   
[Distributed Representations of Words and Phrases and their Compositionality](notes/dist-representation-words.md) - Mikolov et al. [[paper](https://arxiv.org/abs/1310.4546)]      
[GloVe: Global Vectors for Word Representations](notes/glove.md) - Pennington et al [[paper](https://nlp.stanford.edu/pubs/glove.pdf)]    

#### Neural Machine Translation
[Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](notes/learning-phrase-repr.md) - Cho et al. [[paper](https://arxiv.org/abs/1406.1078)]    
[Sequence to Sequence Learning with Neural Networks](notes/seq-to-seq-neural-nets.md) - Sutskever et al. [[paper](https://arxiv.org/abs/1409.3215)]    
[Neural Machine Translation by Jointly Learning To Align and Translate](notes/jointly-learn-to-align-and-translate.md) - Bahdanau et al. [[paper](https://arxiv.org/abs/1409.0473)]  
[Effective Approaches to Attention-based Neural Machine Translation](notes/effective-approach-to-attention-nmt.md) - Luong et al. [[paper](https://arxiv.org/abs/1508.04025)]   
[Google's Neural Machine Translation System: Bridging the Gap between Human and MT](notes/gnmt.md) - Wu et al. [[paper](https://arxiv.org/abs/1609.08144)]   
[Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation](notes/gnmt-zero.md) - Johnson et al. [[paper](http://arxiv.org/pdf/1611.04558.pdf)]    
[Plan, Attend, Generate: Character-Level Neural Machine Translation with Planning](notes/plan-attend-generate.md) - Gulcehre et al. [[paper](https://arxiv.org/abs/1706.05087)]    

#### Text Summarization
[A Hierarchical Neural Autoencoder for Paragraphs and Documents](notes/hier-neural-autoencoder.md) - Li et al. [[paper](https://arxiv.org/abs/1506.01057)]  
[A Neural Attention Model for Abstractive Sentence Summarization](notes/neural-attn-abs-sent-summ.md) - Rush et al. [[paper](https://arxiv.org/abs/1509.00685)]     
[Abstractive Sentence Summarization with Attentive Recurrent Neural Networks](notes/abs-summ-attentive-rec-networks.md) - Chopra et al. [[paper](http://nlp.seas.harvard.edu/papers/naacl16_summary.pdf)]       


#### Image Captioning
[Show and Tell: A Neural Image Caption Generator](notes/show-and-tell.md) - Vinyals et al. [[paper](https://arxiv.org/abs/1411.4555)]   
[Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](notes/show-attend-and-tell.md) - Xu et al. [[paper](https://arxiv.org/abs/1502.03044)]

#### Other
[Pointer Networks](notes/pointer-networks.md) - Vinyals et al. [[paper](https://arxiv.org/abs/1506.03134)]     
[Pointing the Unknown Words](notes/pointing-the-unknown.md) - Gulcehre et al. [[paper](https://arxiv.org/abs/1603.08148)]    
[Pointer Sentinel Mixture Model](notes/pointer-sentinel-mixture.md) - Merity et al. [[paper](https://arxiv.org/abs/1609.07843)]      
[Teaching Machines to Read and Comprehend](notes/teaching-machines-read.md) - Hermann et al. [[paper](https://arxiv.org/abs/1506.03340)]         
