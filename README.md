# Deep Learning Notes
This respository contains short notes on Deep Learning research publications that I read.   
This idea was borrowed from [Denny Britz](https://github.com/dennybritz) and his [notes repository](https://github.com/dennybritz/deeplearning-papernotes).

## Natural Language Processing
#### Distributed Representation/Embeddings
[Linguistic Regularities in Continuous Space Word Representations](notes/regularities-in-cont-word-repr.md) - Mikolov et al. [[paper](https://www.aclweb.org/anthology/N13-1090)]   
[Efficient Estimation of Word Representations in Vector Space](notes/efficient-esti-of-word-repr.md) - Mikolov et al. [[paper](https://arxiv.org/abs/1301.3781)]   
[Distributed Representations of Words and Phrases and their Compositionality](notes/dist-representation-words.md) - Mikolov et al. [[paper](https://arxiv.org/abs/1310.4546)]      
[GloVe: Global Vectors for Word Representations](notes/glove.md) - Pennington et al [[paper](https://nlp.stanford.edu/pubs/glove.pdf)]   
[Skip-Thought Vectors](notes/skip-thought-vectors.md) - Kiros et al [[paper](https://arxiv.org/abs/1506.06726)]    


#### Neural Machine Translation
[Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](notes/learning-phrase-repr.md) - Cho et al. [[paper](https://arxiv.org/abs/1406.1078)]    
[Sequence to Sequence Learning with Neural Networks](notes/seq-to-seq-neural-nets.md) - Sutskever et al. [[paper](https://arxiv.org/abs/1409.3215)]    
[Neural Machine Translation by Jointly Learning To Align and Translate](notes/jointly-learn-to-align-and-translate.md) - Bahdanau et al. [[paper](https://arxiv.org/abs/1409.0473)]  
[Effective Approaches to Attention-based Neural Machine Translation](notes/effective-approach-to-attention-nmt.md) - Luong et al. [[paper](https://arxiv.org/abs/1508.04025)]   
[Google's Neural Machine Translation System: Bridging the Gap between Human and MT](notes/gnmt.md) - Wu et al. [[paper](https://arxiv.org/abs/1609.08144)]   
[Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation](notes/gnmt-zero.md) - Johnson et al. [[paper](http://arxiv.org/pdf/1611.04558.pdf)]    
[Plan, Attend, Generate: Character-Level Neural Machine Translation with Planning](notes/plan-attend-generate.md) - Gulcehre et al. [[paper](https://arxiv.org/abs/1706.05087)]    


#### Text Summarization
[A Hierarchical Neural Autoencoder for Paragraphs and Documents](notes/hier-neural-autoencoder.md) - Li et al. [[paper](https://arxiv.org/abs/1506.01057)]  
[A Neural Attention Model for Abstractive Sentence Summarization](notes/neural-attn-abs-sent-summ.md) - Rush et al. [[paper](https://arxiv.org/abs/1509.00685)]     
[Abstractive Sentence Summarization with Attentive Recurrent Neural Networks](notes/abs-summ-attentive-rec-networks.md) - Chopra et al. [[paper](http://nlp.seas.harvard.edu/papers/naacl16_summary.pdf)]  
[Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond](notes/abstractive-text-sum-rnns-beyond.md) - Nallapati et al. [[paper](https://arxiv.org/abs/1602.06023)]   
[Get To The Point: Summarization with Pointer-Generator Networks](notes/get-to-the-point.md) - See et al. [[paper](https://arxiv.org/abs/1704.04368)]   
[A Deep Reinforced Model for Abstractive Summarization](notes/reinforced-text-sum.md) - Paulus et al. [[paper](https://arxiv.org/abs/1705.04304)]   
[Generating Wikipedia by Summarizing Long Sequences](notes/sum-long-seq.md) - Liu et al. [[paper](https://arxiv.org/abs/1801.10198)]   
[Global Encoding for Abstractive Summarization](notes/glob-enc-abs-sum.md) - Lin et al. [[paper](https://arxiv.org/abs/1805.03989)]   
[A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss](notes/uni-model-for-ext-abs-sum.md) - Hsu et al. [[paper](https://arxiv.org/abs/1805.06266)]   
[Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation](notes/soft-layer-multi-sum.md) - Guo et al. [[paper](https://arxiv.org/abs/1805.11004)]   
[Content Selection in Deep Learning Models of Summarization](notes/content-selection-sum.md) - Kedzie et al. [[paper](https://arxiv.org/abs/1810.12343)]   
[Improving Neural Abstractive Document Summarization with Explicit Information Selection Modeling](notes/abs-sum-info-sel-mod.md) - Li et al. [[paper](http://aclweb.org/anthology/D18-1205)]   
[On the Abstractiveness of Neural Document Summarization](notes/abstractiveness-neural-doc-sum.md) - Zhang  et al. [[paper](http://aclweb.org/anthology/D18-1089)]   
[Neural Latent Extractive Document Summarization](notes/neural-latent-ext-summ.md) - Zhang  et al. [[paper](https://arxiv.org/abs/1808.07187)]   
[Iterative Document Representation Learning Towards Summarization with Polishing](notes/iter-doc-repr-learning.md) - Chen et al. [[paper](https://arxiv.org/abs/1809.10324)]   


#### Image Captioning
[Show and Tell: A Neural Image Caption Generator](notes/show-and-tell.md) - Vinyals et al. [[paper](https://arxiv.org/abs/1411.4555)]   
[Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](notes/show-attend-and-tell.md) - Xu et al. [[paper](https://arxiv.org/abs/1502.03044)]

#### GANs
[Professor Forcing: A New Algorithm for Training Recurrent Networks](notes/professor-forcing.md) - Lamb et al. [[paper](https://arxiv.org/abs/1610.09038)]      
[SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](notes/seqgan.md) - Yu et al. [[paper](https://arxiv.org/abs/1609.05473)]      
[Adversarial Ranking for Language Generation](notes/rankgan.md) - Lin et al. [[paper](https://arxiv.org/abs/1705.11001)]  

#### Other
[Pointer Networks](notes/pointer-networks.md) - Vinyals et al. [[paper](https://arxiv.org/abs/1506.03134)]     
[Pointing the Unknown Words](notes/pointing-the-unknown.md) - Gulcehre et al. [[paper](https://arxiv.org/abs/1603.08148)]    
[Pointer Sentinel Mixture Model](notes/pointer-sentinel-mixture.md) - Merity et al. [[paper](https://arxiv.org/abs/1609.07843)]      
[Teaching Machines to Read and Comprehend](notes/teaching-machines-read.md) - Hermann et al. [[paper](https://arxiv.org/abs/1506.03340)]           
[Quasi-Recurrent Neural Networks](notes/qrnn.md) - Bradbury et al. [[paper](https://arxiv.org/abs/1611.01576)]    
[Twin Networks: Matching The Future For Sequence Generation](notes/twin-networks.md) - Serdyuk et al. [[paper](https://arxiv.org/abs/1708.06742)]        
[Sequence-to-Sequence Learning as Beam-Search Optimization](notes/learning-as-bso.md) - Wiseman et al. [[paper]( https://arxiv.org/abs/1606.02960)]            
[Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks](notes/scheduled-sampling.md) - Bengio et al. [[paper](https://arxiv.org/abs/1506.03099)]   
[Analysis Methods in Neural Language Processing](notes/nlp-analysis.md) - Belinkov et al. [[paper](https://arxiv.org/abs/1812.08951)]     
[Character-Level Language Modeling with Deeper Self-Attention](notes/char-lm-with-deep-self-attn.md) - Al-Rfou et al. [[paper](https://arxiv.org/abs/1808.04444)]     

## General
[Highway Networks](notes/highway-nets.md) - Srivastava et al. [[paper](https://arxiv.org/abs/1505.00387)]    
[Distilling the Knowledge in a Neural Network](notes/distilling-knowledge.md) - Hinton et al. [[paper](https://arxiv.org/abs/1503.02531)]    
